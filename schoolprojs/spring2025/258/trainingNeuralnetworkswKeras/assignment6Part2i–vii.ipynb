{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5DluOCkGK5P2"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install -q keras-tuner\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers, constraints, initializers, callbacks\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    (x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "    x_train = x_train.astype(\"float32\") / 255.0\n",
        "    x_test = x_test.astype(\"float32\") / 255.0\n",
        "    x_train = np.expand_dims(x_train, -1)\n",
        "    x_test = np.expand_dims(x_test, -1)\n",
        "    return train_test_split(x_train, y_train, test_size=0.2, random_state=42), (x_test, y_test)\n",
        "\n",
        "(x_train, x_val, y_train, y_val), (x_test, y_test) = load_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CustomDropout(layers.Layer):\n",
        "    def __init__(self, rate):\n",
        "        super().__init__()\n",
        "        self.rate = rate\n",
        "    def call(self, inputs, training=False):\n",
        "        if training:\n",
        "            return tf.nn.dropout(inputs, rate=self.rate)\n",
        "        return inputs\n",
        "\n",
        "class MaxNormDense(layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.units = units\n",
        "    def build(self, input_shape):\n",
        "        self.kernel = self.add_weight(name=\"kernel\", shape=[input_shape[-1], self.units], initializer=\"random_normal\")\n",
        "        self.bias = self.add_weight(name=\"bias\", shape=[self.units], initializer=\"zeros\")\n",
        "    def call(self, inputs):\n",
        "        normed_kernel = tf.clip_by_norm(self.kernel, clip_norm=1.0, axes=[0])\n",
        "        return tf.matmul(inputs, normed_kernel) + self.bias\n",
        "\n"
      ],
      "metadata": {
        "id": "IJfnGzeZOsEQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class OneCycleScheduler(callbacks.Callback):\n",
        "    def __init__(self, max_lr, total_steps):\n",
        "        self.max_lr = max_lr\n",
        "        self.total_steps = total_steps\n",
        "    def on_train_batch_begin(self, batch, logs=None):\n",
        "        pct = self.model.optimizer.iterations / self.total_steps\n",
        "        lr = self.max_lr * (1 - pct)\n",
        "        keras.backend.set_value(self.model.optimizer.learning_rate, lr)\n",
        "\n",
        "\n",
        "log_dir = \"logs/custom_keras\"\n",
        "tb_cb = callbacks.TensorBoard(log_dir=log_dir)\n",
        "\n",
        "class CustomHuber(keras.losses.Loss):\n",
        "    def __init__(self, delta=1.0):\n",
        "        super().__init__()\n",
        "        self.delta = delta\n",
        "    def call(self, y_true, y_pred):\n",
        "        error = y_true - y_pred\n",
        "        is_small = tf.abs(error) <= self.delta\n",
        "        small_loss = 0.5 * tf.square(error)\n",
        "        big_loss = self.delta * (tf.abs(error) - 0.5 * self.delta)\n",
        "        return tf.where(is_small, small_loss, big_loss)\n",
        "\n"
      ],
      "metadata": {
        "id": "id9hRnd5OuX7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyL1Regularizer(regularizers.Regularizer):\n",
        "    def __init__(self, strength=0.01):\n",
        "        self.strength = strength\n",
        "    def __call__(self, x):\n",
        "        return self.strength * tf.reduce_sum(tf.abs(x))\n",
        "\n",
        "class MyGlorotInit(initializers.Initializer):\n",
        "    def __call__(self, shape, dtype=None):\n",
        "        limit = tf.sqrt(tf.constant(6.0, dtype=tf.float32) / tf.cast(tf.reduce_sum(shape), tf.float32))\n",
        "        return tf.random.uniform(shape, -limit, limit, dtype=dtype)\n",
        "\n",
        "\n",
        "def my_leaky_relu(x):\n",
        "    return tf.nn.leaky_relu(x, alpha=0.2)\n",
        "\n",
        "class MyPositiveWeights(constraints.Constraint):\n",
        "    def __call__(self, w):\n",
        "        return tf.nn.relu(w)\n",
        "\n",
        "class CustomAccuracy(keras.metrics.Metric):\n",
        "    def __init__(self, name=\"custom_accuracy\", **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.total = self.add_weight(name=\"total\", initializer=\"zeros\")\n",
        "        self.count = self.add_weight(name=\"count\", initializer=\"zeros\")\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        values = tf.cast(tf.equal(tf.argmax(y_pred, axis=1), tf.cast(y_true, tf.int64)), tf.float32)\n",
        "        self.total.assign_add(tf.reduce_sum(values))\n",
        "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
        "    def result(self):\n",
        "        return self.total / self.count\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Input(shape=(28, 28, 1)),\n",
        "    layers.Flatten(),\n",
        "    MaxNormDense(128),\n",
        "    layers.Activation(my_leaky_relu),\n",
        "    CustomDropout(0.3),\n",
        "    layers.Dense(64,\n",
        "                 activation=my_leaky_relu,\n",
        "                 kernel_initializer=MyGlorotInit(),\n",
        "                 kernel_regularizer=MyL1Regularizer(),\n",
        "                 kernel_constraint=MyPositiveWeights()),\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "])\n"
      ],
      "metadata": {
        "id": "e0y3qFpnOw6t"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MutableLearningRate(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, initial_lr):\n",
        "        self.lr = tf.Variable(initial_lr, trainable=False, dtype=tf.float32)\n",
        "    def __call__(self, step):\n",
        "        return self.lr\n",
        "\n",
        "mutable_lr = MutableLearningRate(0.001)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=mutable_lr),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[CustomAccuracy()]\n",
        ")\n",
        "\n",
        "\n",
        "model.fit(\n",
        "    x_train, y_train,\n",
        "    validation_data=(x_val, y_val),\n",
        "    epochs=10,\n",
        "    callbacks=[tb_cb, OneCycleScheduler(max_lr=0.001, total_steps=10 * len(x_train) // 32)]\n",
        ")\n",
        "\n",
        "model.evaluate(x_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "becu_9vNOy-i",
        "outputId": "fffa275f-02e2-4199-a057-15af77703175"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/tensorboard.py:680: UserWarning: Model failed to serialize as JSON. Ignoring... <__main__.MyL1Regularizer object at 0x79399118ce10> does not implement get_config()\n",
            "  warnings.warn(f\"Model failed to serialize as JSON. Ignoring... {exc}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 8ms/step - custom_accuracy: 0.6608 - loss: 2.0235 - val_custom_accuracy: 0.8073 - val_loss: 0.7807\n",
            "Epoch 2/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 7ms/step - custom_accuracy: 0.7980 - loss: 0.7886 - val_custom_accuracy: 0.8185 - val_loss: 0.7095\n",
            "Epoch 3/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 8ms/step - custom_accuracy: 0.8135 - loss: 0.7143 - val_custom_accuracy: 0.8278 - val_loss: 0.6577\n",
            "Epoch 4/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 9ms/step - custom_accuracy: 0.8238 - loss: 0.6702 - val_custom_accuracy: 0.8360 - val_loss: 0.6231\n",
            "Epoch 5/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - custom_accuracy: 0.8311 - loss: 0.6390 - val_custom_accuracy: 0.8351 - val_loss: 0.6094\n",
            "Epoch 6/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 9ms/step - custom_accuracy: 0.8309 - loss: 0.6265 - val_custom_accuracy: 0.8412 - val_loss: 0.5892\n",
            "Epoch 7/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - custom_accuracy: 0.8346 - loss: 0.6112 - val_custom_accuracy: 0.8451 - val_loss: 0.5766\n",
            "Epoch 8/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - custom_accuracy: 0.8400 - loss: 0.5911 - val_custom_accuracy: 0.8468 - val_loss: 0.5701\n",
            "Epoch 9/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - custom_accuracy: 0.8416 - loss: 0.5819 - val_custom_accuracy: 0.8469 - val_loss: 0.5633\n",
            "Epoch 10/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - custom_accuracy: 0.8432 - loss: 0.5723 - val_custom_accuracy: 0.8470 - val_loss: 0.5585\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - custom_accuracy: 0.8434 - loss: 0.5698\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5775592923164368, 0.8371000289916992]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}