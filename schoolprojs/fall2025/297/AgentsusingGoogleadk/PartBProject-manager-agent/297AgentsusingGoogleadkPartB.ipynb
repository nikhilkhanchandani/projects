{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2d1b810754584e729968cdcfa9be6d26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d98e9b66b44c4f3b9a341ff297c2f864",
              "IPY_MODEL_29e3eb23a912438796c3b4742d749d7c",
              "IPY_MODEL_de271298868141019bbf7bbcba3fe81f"
            ],
            "layout": "IPY_MODEL_774348fcb5204aa49a7c069fec53fc14"
          }
        },
        "d98e9b66b44c4f3b9a341ff297c2f864": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ab23cd919614ed5b9e05db398679141",
            "placeholder": "​",
            "style": "IPY_MODEL_4d7bb5e7ea1045d2953a49d6a22008be",
            "value": "deepseek-coder-6.7b-instruct.Q4_K_M.gguf: 100%"
          }
        },
        "29e3eb23a912438796c3b4742d749d7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed139cd20af143c2bd253dd24e07e31a",
            "max": 4083015904,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e1e894f27aa64401b8e80357716fddb2",
            "value": 4083015904
          }
        },
        "de271298868141019bbf7bbcba3fe81f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_230961533a87480c864c3a4aaef30e07",
            "placeholder": "​",
            "style": "IPY_MODEL_abeaa7587dea412d8fa8610d7e0a2eb1",
            "value": " 4.08G/4.08G [00:56&lt;00:00, 107MB/s]"
          }
        },
        "774348fcb5204aa49a7c069fec53fc14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ab23cd919614ed5b9e05db398679141": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d7bb5e7ea1045d2953a49d6a22008be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed139cd20af143c2bd253dd24e07e31a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1e894f27aa64401b8e80357716fddb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "230961533a87480c864c3a4aaef30e07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abeaa7587dea412d8fa8610d7e0a2eb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUbjXprBehb0",
        "outputId": "03b3535f-2d96-40a3-853f-d116c9caecad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m123.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m112.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.1/278.1 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip -q install --upgrade \"llama-cpp-python[server]==0.2.84\" google-adk litellm pandas matplotlib python-dateutil\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "import os\n",
        "\n",
        "REPO_ID  = \"TheBloke/DeepSeek-Coder-6.7B-Instruct-GGUF\"\n",
        "FILENAME = \"deepseek-coder-6.7b-instruct.Q4_K_M.gguf\"   # ~3.8 GB\n",
        "\n",
        "model_path = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)\n",
        "os.environ[\"MODEL_PATH\"] = model_path\n",
        "print(\"MODEL_PATH =\", os.environ[\"MODEL_PATH\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170,
          "referenced_widgets": [
            "2d1b810754584e729968cdcfa9be6d26",
            "d98e9b66b44c4f3b9a341ff297c2f864",
            "29e3eb23a912438796c3b4742d749d7c",
            "de271298868141019bbf7bbcba3fe81f",
            "774348fcb5204aa49a7c069fec53fc14",
            "6ab23cd919614ed5b9e05db398679141",
            "4d7bb5e7ea1045d2953a49d6a22008be",
            "ed139cd20af143c2bd253dd24e07e31a",
            "e1e894f27aa64401b8e80357716fddb2",
            "230961533a87480c864c3a4aaef30e07",
            "abeaa7587dea412d8fa8610d7e0a2eb1"
          ]
        },
        "id": "ksNA2aZz5I23",
        "outputId": "57234c4e-fca0-4531-c42b-b91c4390f79d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "deepseek-coder-6.7b-instruct.Q4_K_M.gguf:   0%|          | 0.00/4.08G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d1b810754584e729968cdcfa9be6d26"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MODEL_PATH = /root/.cache/huggingface/hub/models--TheBloke--DeepSeek-Coder-6.7B-Instruct-GGUF/snapshots/9e221e6b41cb1bf1c5d8f9718e81e3dc781f7557/deepseek-coder-6.7b-instruct.Q4_K_M.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# 3.A1 — start server in background and log to /content/server.log\n",
        "pkill -f \"llama_cpp.server.*--port 8000\" || true\n",
        "\n",
        "nohup python -m llama_cpp.server \\\n",
        "  --model \"$MODEL_PATH\" \\\n",
        "  --host 127.0.0.1 --port 8000 \\\n",
        "  --model_alias deepseek-local \\\n",
        "  --chat_format chatml \\\n",
        "  --n_ctx 4096 \\\n",
        "  > /content/server.log 2>&1 &\n",
        "\n",
        "sleep 3\n",
        "tail -n 50 /content/server.log\n"
      ],
      "metadata": {
        "id": "JbWRVDGe5Kcw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.A2 — wait for server to be ready\n",
        "import time, requests, subprocess\n",
        "\n",
        "base = \"http://127.0.0.1:8000\"\n",
        "for i in range(120):  # up to ~6 minutes on CPU\n",
        "    try:\n",
        "        r = requests.get(f\"{base}/v1/models\", timeout=2)\n",
        "        if r.ok:\n",
        "            print(\"Server is ready ✅\")\n",
        "            print(r.json())\n",
        "            break\n",
        "    except Exception:\n",
        "        pass\n",
        "    if i % 5 == 0:\n",
        "        print(f\"...waiting ({i*3}s)\")\n",
        "    time.sleep(3)\n",
        "else:\n",
        "    print(\"Server not ready; last 100 log lines:\")\n",
        "    subprocess.run([\"tail\",\"-n\",\"100\",\"/content/server.log\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihUA5bgK5NEv",
        "outputId": "c652f875-3f34-4854-a96e-2ac076b576ca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...waiting (0s)\n",
            "Server is ready ✅\n",
            "{'object': 'list', 'data': [{'id': 'deepseek-local', 'object': 'model', 'owned_by': 'me', 'permissions': []}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass, asdict\n",
        "from datetime import datetime, timedelta\n",
        "from typing import List, Dict, Any, Optional\n",
        "import json, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "STORE = Path(\"/content/pm_tasks.json\")\n",
        "\n",
        "@dataclass\n",
        "class Task:\n",
        "    id: int\n",
        "    title: str\n",
        "    owner: str\n",
        "    status: str          # \"todo\"|\"doing\"|\"blocked\"|\"done\"\n",
        "    estimate_h: float    # original estimate\n",
        "    spent_h: float       # time spent so far\n",
        "    due: str             # ISO date\n",
        "    tags: List[str]\n",
        "\n",
        "def _load() -> List[Dict[str, Any]]:\n",
        "    if STORE.exists():\n",
        "        return json.loads(STORE.read_text())\n",
        "    return []\n",
        "\n",
        "def _save(rows: List[Dict[str, Any]]):\n",
        "    STORE.write_text(json.dumps(rows, indent=2))\n",
        "\n",
        "def seed_sample():\n",
        "    if STORE.exists():\n",
        "        return\n",
        "    today = datetime.now().date()\n",
        "    rows = [\n",
        "        asdict(Task(1,\"Auth flow\",\"Aisha\",\"doing\",8,3,(today+timedelta(days=3)).isoformat(),[\"backend\"])),\n",
        "        asdict(Task(2,\"Landing page\",\"Miguel\",\"todo\",6,0,(today+timedelta(days=2)).isoformat(),[\"frontend\"])),\n",
        "        asdict(Task(3,\"Billing webhook\",\"Aisha\",\"blocked\",10,2,(today+timedelta(days=5)).isoformat(),[\"backend\",\"payments\"])),\n",
        "        asdict(Task(4,\"Onboarding email\",\"Nora\",\"done\",3,3,(today+timedelta(days=1)).isoformat(),[\"growth\"])),\n",
        "        asdict(Task(5,\"DB backup setup\",\"Ravi\",\"doing\",5,1,(today+timedelta(days=4)).isoformat(),[\"infra\"])),\n",
        "    ]\n",
        "    _save(rows)\n",
        "\n",
        "def list_tasks(owner: Optional[str]=None, status: Optional[str]=None) -> Dict[str, Any]:\n",
        "    rows = _load()\n",
        "    if owner: rows = [r for r in rows if r[\"owner\"].lower()==owner.lower()]\n",
        "    if status: rows = [r for r in rows if r[\"status\"].lower()==status.lower()]\n",
        "    return {\"tasks\": rows}\n",
        "\n",
        "def create_task(title:str, owner:str, estimate_h:float, due:str, tags:List[str]) -> Dict[str, Any]:\n",
        "    rows = _load()\n",
        "    new_id = (max([r[\"id\"] for r in rows]) + 1) if rows else 1\n",
        "    t = asdict(Task(new_id, title, owner, \"todo\", float(estimate_h), 0.0, due, tags))\n",
        "    rows.append(t); _save(rows)\n",
        "    return {\"created\": t}\n",
        "\n",
        "def update_status(task_id:int, status:str, spent_delta:float=0.0) -> Dict[str, Any]:\n",
        "    rows = _load(); found = None\n",
        "    for r in rows:\n",
        "        if r[\"id\"]==task_id:\n",
        "            r[\"status\"] = status\n",
        "            r[\"spent_h\"] = max(0.0, r[\"spent_h\"] + float(spent_delta))\n",
        "            found = r; break\n",
        "    if found is None: return {\"error\":\"not_found\"}\n",
        "    _save(rows)\n",
        "    return {\"updated\": found}\n",
        "\n",
        "def compute_summary() -> Dict[str, Any]:\n",
        "    rows = _load()\n",
        "    df = pd.DataFrame(rows)\n",
        "    if df.empty:\n",
        "        return {\"counts\":{}, \"burndown_points\":[], \"eta_days\": None}\n",
        "    counts = df[\"status\"].value_counts().to_dict()\n",
        "    total_est = df[\"estimate_h\"].sum()\n",
        "    total_spent = df[\"spent_h\"].sum()\n",
        "    remaining = max(0.0, total_est - total_spent)\n",
        "    # naive ETA: remaining / avg_daily_throughput (assume 6h per dev-day)\n",
        "    eta_days = round(remaining / 6.0, 1)\n",
        "    # simple burndown (fake historic points): remaining declines linearly over 7 days\n",
        "    pts = []\n",
        "    for d in range(7, -1, -1):\n",
        "        pts.append({\"day\": (datetime.now().date()-timedelta(days=d)).isoformat(),\n",
        "                    \"remaining_h\": max(0.0, remaining + (d* (total_est/7.0 - total_spent/7.0)))})\n",
        "    return {\"counts\": counts, \"remaining_h\": remaining, \"eta_days\": eta_days, \"burndown_points\": pts}\n",
        "\n",
        "seed_sample()\n",
        "print(\"Seeded:\", len(_load()), \"tasks\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asGk0jw55Thu",
        "outputId": "f3bd0b7f-5e60-4b3a-dd31-1adc34cdf3ec"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seeded: 5 tasks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_burndown(points: List[Dict[str, Any]], out_path: Path) -> str:\n",
        "    if not points:\n",
        "        return \"\"\n",
        "    xs = [p[\"day\"] for p in points]\n",
        "    ys = [p[\"remaining_h\"] for p in points]\n",
        "    plt.figure(figsize=(7,4))\n",
        "    plt.plot(xs, ys, marker=\"o\")\n",
        "    plt.title(\"Burndown (Remaining Hours)\")\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.tight_layout()\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    plt.savefig(out_path)\n",
        "    plt.close()\n",
        "    return str(out_path)\n"
      ],
      "metadata": {
        "id": "RalBmAiY6zQQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json\n",
        "from google.adk.agents import LlmAgent\n",
        "from google.adk.models.lite_llm import LiteLlm\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.genai import types\n",
        "from pathlib import Path\n",
        "\n",
        "# model: your local llama.cpp alias\n",
        "composer = LlmAgent(\n",
        "    model=LiteLlm(model=\"openai/deepseek-local\"),\n",
        "    name=\"pm_status_reporter\",\n",
        "    description=\"Writes a concise project status report from tool outputs.\",\n",
        "    instruction=(\n",
        "        \"You are a project manager. Given JSON of tasks and a summary, produce a clear Markdown status report with:\\n\"\n",
        "        \"1) Summary KPIs (counts by status, remaining hours, ETA)\\n\"\n",
        "        \"2) Owner breakdown (bullets)\\n\"\n",
        "        \"3) Top risks/blockers (from any 'blocked' tasks)\\n\"\n",
        "        \"4) Upcoming deadlines (next 3 by due date)\\n\"\n",
        "        \"5) Next actions (bullets)\\n\"\n",
        "        \"Keep it to ~250–350 words. Do not apologize or refuse. Use only provided numbers.\"\n",
        "    ),\n",
        "    tools=[],\n",
        ")\n",
        "\n",
        "APP_NAME=\"adk_pm_demo\"\n",
        "USER_ID=\"student\"\n",
        "SESSION_ID=\"session-pm-001\"\n",
        "\n",
        "def build_pack() -> Dict[str, Any]:\n",
        "    rows = _load()\n",
        "    owners = sorted(set(r[\"owner\"] for r in rows))\n",
        "    summary = compute_summary()\n",
        "    upcoming = sorted(rows, key=lambda r: r[\"due\"])[:3]\n",
        "    blocked = [r for r in rows if r[\"status\"]==\"blocked\"]\n",
        "    return {\"owners\": owners, \"tasks\": rows, \"summary\": summary, \"upcoming\": upcoming, \"blocked\": blocked}\n",
        "\n",
        "async def generate_status_report() -> str:\n",
        "    pack = build_pack()\n",
        "    user_msg = (\n",
        "        \"Write a project status report from this JSON. Output Markdown only.\\n\\n\"\n",
        "        f\"{json.dumps(pack, indent=2)}\"\n",
        "    )\n",
        "\n",
        "    # try ADK; fallback if refusal/timeout\n",
        "    session_service = InMemorySessionService()\n",
        "    await session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
        "    runner = Runner(agent=composer, app_name=APP_NAME, session_service=session_service)\n",
        "\n",
        "    content = types.Content(role=\"user\", parts=[types.Part(text=user_msg)])\n",
        "    final_text = None; acc = \"\"\n",
        "\n",
        "    try:\n",
        "        for event in runner.run(user_id=USER_ID, session_id=SESSION_ID, new_message=content):\n",
        "            if getattr(event, \"content\", None) and getattr(event.content, \"parts\", None):\n",
        "                t = getattr(event.content.parts[0], \"text\", None)\n",
        "                if t:\n",
        "                    acc += t; final_text = acc\n",
        "                    if len(acc) > 1800: break\n",
        "    except Exception as e:\n",
        "        print(\"[LLM error] Fallback:\", repr(e))\n",
        "        final_text = None\n",
        "\n",
        "    # simple fallback renderer\n",
        "    bad = (not final_text) or any(k in final_text.lower() for k in [\"i'm sorry\",\"as an ai\",\"cannot\",\"unable\"])\n",
        "    if bad:\n",
        "        s = pack[\"summary\"]; counts = s.get(\"counts\", {})\n",
        "        lines = [\n",
        "            \"# Project Status Report\",\n",
        "            f\"**KPI** — todo:{counts.get('todo',0)}, doing:{counts.get('doing',0)}, \"\n",
        "            f\"blocked:{counts.get('blocked',0)}, done:{counts.get('done',0)}; \"\n",
        "            f\"remaining: {int(s.get('remaining_h',0))}h; ETA: {s.get('eta_days','?')} days\",\n",
        "            \"## Owner Breakdown\",\n",
        "        ]\n",
        "        by_owner = {}\n",
        "        for r in pack[\"tasks\"]:\n",
        "            by_owner.setdefault(r[\"owner\"], []).append(r)\n",
        "        for o, rows in by_owner.items():\n",
        "            lines.append(f\"- **{o}**: \" + \", \".join(f\"{r['title']}({r['status']})\" for r in rows[:5]))\n",
        "        if pack[\"blocked\"]:\n",
        "            lines.append(\"## Risks/Blockers\")\n",
        "            for r in pack[\"blocked\"]:\n",
        "                lines.append(f\"- #{r['id']} {r['title']} — owner {r['owner']}, due {r['due']}\")\n",
        "        if pack[\"upcoming\"]:\n",
        "            lines.append(\"## Upcoming (next 3)\")\n",
        "            for r in pack[\"upcoming\"]:\n",
        "                lines.append(f\"- #{r['id']} {r['title']} — {r['due']}\")\n",
        "        lines += [\"## Next Actions\", \"- Unblock webhook\", \"- Finish auth flow\", \"- Prepare release notes\"]\n",
        "        final_text = \"\\n\\n\".join(lines)\n",
        "\n",
        "    # save artifacts\n",
        "    out_dir = Path(\"/content/agent_outputs\"); out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    md_path = out_dir / f\"pm_status_{datetime.now().strftime('%Y%m%d-%H%M%S')}.md\"\n",
        "    md_path.write_text(final_text)\n",
        "\n",
        "    # chart\n",
        "    bd = pack[\"summary\"].get(\"burndown_points\", [])\n",
        "    chart_path = out_dir / f\"pm_burndown_{datetime.now().strftime('%Y%m%d-%H%M%S')}.png\"\n",
        "    chart_file = plot_burndown(bd, chart_path) if bd else \"\"\n",
        "\n",
        "    print(\"Saved:\", md_path)\n",
        "    if chart_file: print(\"Chart:\", chart_file)\n",
        "    return final_text\n",
        "\n",
        "# Quick smoke test\n",
        "print(\"Tasks now:\", len(_load()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAmyDVB362AL",
        "outputId": "bdb6fb53-a390-4277-8170-74223d52a2e5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tasks now: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, requests\n",
        "\n",
        "# Point ADK/LiteLLM to your local llama.cpp server\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"http://127.0.0.1:8000/v1\"\n",
        "os.environ[\"OPENAI_API_KEY\"]  = \"sk-local-anything\"  # any string works for local\n",
        "\n",
        "# Optional: shorter timeouts and fail-fast\n",
        "os.environ[\"LITELLM_TIMEOUT\"] = \"120\"\n",
        "os.environ[\"LITELLM_REQUEST_TIMEOUT\"] = \"120\"\n",
        "os.environ[\"LITELLM_MAX_RETRIES\"] = \"0\"\n",
        "\n",
        "# Sanity probe (optional)\n",
        "try:\n",
        "    r = requests.get(\"http://127.0.0.1:8000/v1/models\", timeout=5)\n",
        "    print(\"Models:\", r.json())\n",
        "except Exception as e:\n",
        "    print(\"Server not reachable yet:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Snl2hWJ9V_v",
        "outputId": "7ae02f0a-82a0-4276-d1b0-84ae2e6b8a5d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models: {'object': 'list', 'data': [{'id': 'deepseek-local', 'object': 'model', 'owned_by': 'me', 'permissions': []}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an extra task & update one, to show tool usage\n",
        "_ = create_task(\"QA test plan\",\"Nora\", 4, (datetime.now().date()+timedelta(days=2)).isoformat(), [\"qa\"])\n",
        "_ = update_status(3, \"doing\", spent_delta=1.0)  # unblock a task\n",
        "report_md = await generate_status_report()\n",
        "print(report_md[:1200])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDL6_Dmk7ZWj",
        "outputId": "09b4a331-1d9f-4332-d335-cd6168bb1f26"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/agent_outputs/pm_status_20251023-030550.md\n",
            "Chart: /content/agent_outputs/pm_burndown_20251023-030550.png\n",
            "# Project Status Report\n",
            "\n",
            "**KPI** — todo:3, doing:3, blocked:0, done:1; remaining: 29h; ETA: 4.8 days\n",
            "\n",
            "## Owner Breakdown\n",
            "\n",
            "- **Aisha**: Auth flow(doing), Billing webhook(doing)\n",
            "\n",
            "- **Miguel**: Landing page(todo)\n",
            "\n",
            "- **Nora**: Onboarding email(done), QA test plan(todo), QA test plan(todo)\n",
            "\n",
            "- **Ravi**: DB backup setup(doing)\n",
            "\n",
            "## Upcoming (next 3)\n",
            "\n",
            "- #4 Onboarding email — 2025-10-24\n",
            "\n",
            "- #2 Landing page — 2025-10-25\n",
            "\n",
            "- #6 QA test plan — 2025-10-25\n",
            "\n",
            "## Next Actions\n",
            "\n",
            "- Unblock webhook\n",
            "\n",
            "- Finish auth flow\n",
            "\n",
            "- Prepare release notes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UI: Project Manager Agent (Gradio)\n",
        "!pip -q install gradio nest_asyncio\n",
        "\n",
        "import gradio as gr\n",
        "import nest_asyncio, asyncio\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "def _tasks_df():\n",
        "    try:\n",
        "        rows = list_tasks()[\"tasks\"]\n",
        "    except Exception:\n",
        "        rows = []\n",
        "    if not rows:\n",
        "        return pd.DataFrame([{\"info\": \"No tasks yet\"}])\n",
        "    df = pd.DataFrame(rows)\n",
        "    # nicer column order\n",
        "    cols = [\"id\",\"title\",\"owner\",\"status\",\"estimate_h\",\"spent_h\",\"due\",\"tags\"]\n",
        "    return df[[c for c in cols if c in df.columns]]\n",
        "\n",
        "def ui_list_tasks():\n",
        "    df = _tasks_df()\n",
        "    return df\n",
        "\n",
        "def ui_create_task(title, owner, estimate_h, due, tags_csv):\n",
        "    tags = [t.strip() for t in (tags_csv or \"\").split(\",\") if t.strip()]\n",
        "    res = create_task(title, owner, estimate_h, due, tags)\n",
        "    t = res.get(\"created\", {})\n",
        "    msg = f\"✅ Created task #{t.get('id')} — {t.get('title')} (owner: {t.get('owner')}, due: {t.get('due')})\"\n",
        "    return msg, _tasks_df()\n",
        "\n",
        "def ui_update_status(task_id, status, spent_delta):\n",
        "    try:\n",
        "        res = update_status(int(task_id), status, float(spent_delta or 0))\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error: {e}\", _tasks_df()\n",
        "    if \"updated\" in res:\n",
        "        u = res[\"updated\"]\n",
        "        msg = (f\"✅ Updated #{u['id']} to {u['status']}; \"\n",
        "               f\"spent_h now {u['spent_h']}\")\n",
        "    else:\n",
        "        msg = \"❌ Task not found.\"\n",
        "    return msg, _tasks_df()\n",
        "\n",
        "# run the async reporter safely from Gradio\n",
        "def ui_generate_report():\n",
        "    loop = asyncio.get_event_loop()\n",
        "    if loop.is_running():\n",
        "        # we are in a notebook; create a task and wait on it\n",
        "        fut = asyncio.ensure_future(generate_status_report())\n",
        "        loop.run_until_complete(asyncio.sleep(0))  # yield once\n",
        "        loop.run_until_complete(fut)\n",
        "        md = fut.result()\n",
        "    else:\n",
        "        md = loop.run_until_complete(generate_status_report())\n",
        "\n",
        "    # find the latest files\n",
        "    out_dir = Path(\"/content/agent_outputs\")\n",
        "    img_path = \"\"\n",
        "    if out_dir.exists():\n",
        "        pngs = sorted(out_dir.glob(\"pm_burndown_*.png\"), reverse=True)\n",
        "        if pngs:\n",
        "            img_path = str(pngs[0])\n",
        "    return md, img_path\n",
        "\n",
        "with gr.Blocks(title=\"Project Manager Agent (ADK)\") as demo:\n",
        "    gr.Markdown(\"# Project Manager Agent (ADK)\\nManage tasks and generate a status report with a local DeepSeek model.\")\n",
        "\n",
        "    with gr.Tab(\"Overview\"):\n",
        "        btn_refresh = gr.Button(\"🔄 Refresh\")\n",
        "        grid = gr.Dataframe(value=_tasks_df(), interactive=False, wrap=True)\n",
        "        btn_refresh.click(fn=ui_list_tasks, outputs=grid)\n",
        "\n",
        "    with gr.Tab(\"Create Task\"):\n",
        "        with gr.Row():\n",
        "            title = gr.Textbox(label=\"Title\", placeholder=\"Implement login form\")\n",
        "            owner = gr.Textbox(label=\"Owner\", placeholder=\"Aisha\")\n",
        "        with gr.Row():\n",
        "            estimate = gr.Number(label=\"Estimate (hours)\", value=4, precision=1)\n",
        "            due = gr.Textbox(label=\"Due Date (YYYY-MM-DD)\", value=str(datetime.now().date()+timedelta(days=3)))\n",
        "            tags = gr.Textbox(label=\"Tags (comma-separated)\", value=\"frontend\")\n",
        "        btn_create = gr.Button(\"➕ Create\")\n",
        "        out_msg_create = gr.Markdown()\n",
        "        out_grid_after_create = gr.Dataframe(interactive=False, wrap=True)\n",
        "        btn_create.click(\n",
        "            fn=ui_create_task,\n",
        "            inputs=[title, owner, estimate, due, tags],\n",
        "            outputs=[out_msg_create, out_grid_after_create]\n",
        "        )\n",
        "\n",
        "    with gr.Tab(\"Update Status\"):\n",
        "        with gr.Row():\n",
        "            task_id = gr.Number(label=\"Task ID\", precision=0, value=1)\n",
        "            status = gr.Dropdown(choices=[\"todo\",\"doing\",\"blocked\",\"done\"], value=\"doing\", label=\"New Status\")\n",
        "            spent_delta = gr.Number(label=\"Add Spent (hours)\", value=1.0, precision=1)\n",
        "        btn_update = gr.Button(\"✅ Update\")\n",
        "        out_msg_update = gr.Markdown()\n",
        "        out_grid_after_update = gr.Dataframe(interactive=False, wrap=True)\n",
        "        btn_update.click(\n",
        "            fn=ui_update_status,\n",
        "            inputs=[task_id, status, spent_delta],\n",
        "            outputs=[out_msg_update, out_grid_after_update]\n",
        "        )\n",
        "\n",
        "    with gr.Tab(\"Generate Report\"):\n",
        "        gr.Markdown(\"Creates a Markdown status report and a burndown chart image in `/content/agent_outputs`.\")\n",
        "        btn_report = gr.Button(\"📝 Generate Status Report\")\n",
        "        report_md = gr.Markdown()\n",
        "        chart_img = gr.Image(label=\"Burndown\", show_label=True)\n",
        "        btn_report.click(fn=ui_generate_report, outputs=[report_md, chart_img])\n",
        "\n",
        "demo.launch(share=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "3dzSytX1LUS6",
        "outputId": "44d140cb-826a-4228-976e-192643227182"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7860, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}